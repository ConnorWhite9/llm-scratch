# Base config: override in train_small.yaml, train_medium.yaml, etc.

model:
  n_layers: 4
  n_heads: 4
  d_model: 256
  d_ff: 1024
  vocab_size: 50257  # set after tokenizer is trained
  max_seq_len: 512
  dropout: 0.1

training:
  batch_size: 32
  max_steps: 10000
  learning_rate: 3.0e-4
  weight_decay: 0.01
  warmup_steps: 500

data:
  processed_dir: data/processed
  seq_length: 512
