# generate(prompt_ids, model, tokenizer, max_new_tokens, temperature, top_k, top_p)
